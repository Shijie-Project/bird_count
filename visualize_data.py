import glob
import os
import warnings

import cv2
import numpy as np
import streamlit as st
import torch
import torch.nn.functional as F
from PIL import Image
from torchvision import transforms

# å°è¯•å¯¼å…¥æ¨¡å‹ï¼Œç¡®ä¿é¡¹ç›®ç»“æ„æ­£ç¡®
from models.shufflenet import get_shufflenet_density_model


warnings.filterwarnings("ignore")

# ================= é…ç½®åŒºåŸŸ =================
DEFAULT_DATA_ROOT = "./data/"
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# ===========================================

st.set_page_config(layout="wide", page_title="Bird Annotation Viewer & Inference")

# --- é¢„å¤„ç† (ä¸è®­ç»ƒä¿æŒä¸€è‡´) ---
# Mean: [0.485, 0.456, 0.406], Std: [0.229, 0.224, 0.225]
tf_normalize = transforms.Compose(
    [transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]
)


def load_data(root_path, split):
    """åŠ è½½æŒ‡å®š split ä¸‹çš„æ‰€æœ‰å›¾ç‰‡è·¯å¾„"""
    img_dir = os.path.join(root_path, "images", split)
    if not os.path.exists(img_dir):
        return []
    extensions = ["*.jpg", "*.png", "*.jpeg"]
    img_files = []
    for ext in extensions:
        img_files.extend(glob.glob(os.path.join(img_dir, ext)))
    return sorted(img_files)


def load_annotation(img_path, root_path, split):
    """æ ¹æ®å›¾ç‰‡è·¯å¾„åŠ è½½å¯¹åº”çš„ .txt æ ‡æ³¨"""
    file_name = os.path.basename(img_path).rsplit(".", 1)[0]
    anno_path = os.path.join(root_path, "annotations", split, file_name + ".txt")
    points = None
    if os.path.exists(anno_path):
        try:
            data = np.loadtxt(anno_path, ndmin=2)
            if data.size > 0:
                points = data
        except Exception:
            pass
    return points, anno_path


@st.cache_resource
def load_model_cached(model_path):
    """åŠ è½½æ¨¡å‹å¹¶ç¼“å­˜ï¼Œé¿å…é‡å¤åŠ è½½"""
    print(f"Loading model from {model_path}...")
    model = get_shufflenet_density_model()
    checkpoint = torch.load(model_path, map_location=DEVICE)

    # å…¼å®¹ checkpoint å­—å…¸æˆ–ç›´æ¥ state_dict
    if isinstance(checkpoint, dict) and "model" in checkpoint:
        state_dict = checkpoint["model"]
    else:
        state_dict = checkpoint

    model.load_state_dict(state_dict, strict=True)
    model.to(DEVICE)
    model.eval()
    return model


def run_inference(model, img_rgb):
    """è¿è¡Œæ¨ç†å¹¶è¿”å› é¢„æµ‹è®¡æ•° å’Œ çƒ­åŠ›å›¾"""
    # 1. é¢„å¤„ç†
    orig_h, orig_w = img_rgb.shape[:2]
    pil_img = Image.fromarray(img_rgb)
    img_tensor = tf_normalize(pil_img).unsqueeze(0).to(DEVICE)

    # Padding åˆ° 32 çš„å€æ•° (ShuffleNetè¦æ±‚)
    h, w = img_tensor.shape[2:]
    ph, pw = 0, 0
    if h % 32 != 0:
        ph = 32 - h % 32
    if w % 32 != 0:
        pw = 32 - w % 32
    if ph > 0 or pw > 0:
        img_tensor = F.pad(img_tensor, (0, pw, 0, ph))

    # 2. æ¨ç†
    with torch.no_grad():
        mu, _ = model(img_tensor)

    pred_count = torch.sum(mu).item()

    # 3. ç”Ÿæˆçƒ­åŠ›å›¾
    density_map = mu.squeeze().cpu().numpy()

    # å½’ä¸€åŒ– (0~1)
    if density_map.max() > 0:
        norm_density = (density_map - density_map.min()) / (density_map.max() - density_map.min() + 1e-6)
    else:
        norm_density = density_map

    # Resize åˆ°åŸå›¾å¤§å°
    norm_density = cv2.resize(norm_density, (orig_w, orig_h), interpolation=cv2.INTER_CUBIC)

    # è½¬ä¸º uint8
    norm_uint8 = (255 * norm_density).astype(np.uint8)

    # åº”ç”¨ JET Colormap
    colormap = cv2.applyColorMap(norm_uint8, cv2.COLORMAP_JET)
    colormap_rgb = cv2.cvtColor(colormap, cv2.COLOR_BGR2RGB)

    # 4. ã€å…³é”®ä¿®å¤ã€‘ä½¿ç”¨ mask èåˆï¼Œå»é™¤è“è‰²èƒŒæ™¯
    threshold = 0.02  # é˜ˆå€¼ï¼Œå¯æ ¹æ®æ•ˆæœå¾®è°ƒ (0.01 ~ 0.1)
    mask = norm_density > threshold  # åªæœ‰å¤§äºé˜ˆå€¼çš„åŒºåŸŸæ‰æ˜¾ç¤ºçƒ­åŠ›å›¾

    overlay = img_rgb.copy()

    if mask.any():
        # åªåœ¨ mask åŒºåŸŸæ··åˆ: åŸå›¾ 0.6 + çƒ­åŠ›å›¾ 0.4
        roi_orig = img_rgb[mask]
        roi_heat = colormap_rgb[mask]
        blended_roi = cv2.addWeighted(roi_orig, 0.6, roi_heat, 0.4, 0)
        overlay[mask] = blended_roi

    return pred_count, overlay


# --- Sidebar: è®¾ç½® ---
st.sidebar.title("ğŸ› ï¸ è®¾ç½®")
data_root = st.sidebar.text_input("æ•°æ®é›†æ ¹ç›®å½•", value=DEFAULT_DATA_ROOT)
split = st.sidebar.selectbox("æ•°æ®åˆ’åˆ† (Split)", ["train", "val", "test"])

# --- åŠ è½½æ•°æ® ---
image_files = load_data(data_root, split)
total_images = len(image_files)

if total_images == 0:
    st.error(f"âŒ åœ¨ `{os.path.join(data_root, split, 'images')}` ä¸‹æœªæ‰¾åˆ°å›¾ç‰‡ï¼")
    st.stop()

# --- Session State ---
if "idx" not in st.session_state:
    st.session_state.idx = 0
if "jump_val" not in st.session_state:
    st.session_state.jump_val = 1
# å­˜å‚¨é¢„æµ‹ç»“æœï¼Œåˆ‡æ¢å›¾ç‰‡æ—¶æ¸…ç©º
if "pred_result" not in st.session_state:
    st.session_state.pred_result = None
if "last_processed_idx" not in st.session_state:
    st.session_state.last_processed_idx = -1

# è¾¹ç•Œæ£€æŸ¥
if st.session_state.idx >= total_images:
    st.session_state.idx = total_images - 1
if st.session_state.idx < 0:
    st.session_state.idx = 0


# --- çŠ¶æ€æ›´æ–°å‡½æ•° ---
def clear_pred():
    """åˆ‡æ¢å›¾ç‰‡æ—¶æ¸…é™¤é¢„æµ‹ç»“æœ"""
    st.session_state.pred_result = None


def update_index(new_idx):
    st.session_state.idx = new_idx
    st.session_state.jump_val = new_idx + 1
    clear_pred()


def next_img():
    update_index((st.session_state.idx + 1) % total_images)


def prev_img():
    update_index((st.session_state.idx - 1) % total_images)


def search_img():
    query = st.session_state.search_query.strip()
    for i, path in enumerate(image_files):
        if query in os.path.basename(path):
            update_index(i)
            return
    st.toast(f"âš ï¸ æœªæ‰¾åˆ° '{query}'", icon="ğŸ”")


def jump_to_index():
    try:
        new_idx = st.session_state.jump_val - 1
        if 0 <= new_idx < total_images:
            st.session_state.idx = new_idx
            clear_pred()
        else:
            st.toast("âš ï¸ ç´¢å¼•è¶Šç•Œ", icon="âŒ")
            st.session_state.jump_val = st.session_state.idx + 1
    except Exception:
        pass


# --- ä¸»ç•Œé¢ ---
col1, col2 = st.columns([3, 1])

# è·å–å½“å‰å›¾ç‰‡
curr_img_path = image_files[st.session_state.idx]
curr_name = os.path.basename(curr_img_path)
points, anno_path = load_annotation(curr_img_path, data_root, split)

with col1:
    st.subheader(f"ğŸ–¼ï¸ {curr_name} ({st.session_state.idx + 1}/{total_images})")

    # è¯»å–å¹¶æ˜¾ç¤º Ground Truth
    img_cv = cv2.imread(curr_img_path)
    if img_cv is not None:
        h, w, c = img_cv.shape
        img_rgb = cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB)

        # ç»˜å›¾é€»è¾‘
        disp_img = img_rgb.copy()
        point_radius = st.sidebar.slider("GT ç‚¹å¤§å°", 1, 10, 4)

        # è½¬æ¢åæ ‡
        gt_points_abs = points
        if points is not None and points.shape[1] == 3:  # å‡è®¾æ˜¯ class, x, y å½’ä¸€åŒ–æ ¼å¼
            gt_points_abs = points[:, 1:] * np.array([w, h])

        # ç”»ç‚¹
        for p in gt_points_abs:
            cx, cy = int(p[0]), int(p[1])
            cv2.circle(disp_img, (cx, cy), point_radius, (255, 0, 0), -1)
            cv2.circle(disp_img, (cx, cy), point_radius + 2, (255, 255, 255), 1)

        # Tab æ˜¾ç¤º: åŸå§‹/GT vs é¢„æµ‹ç»“æœ
        tab_gt, tab_pred = st.tabs(["ğŸ“Œ Ground Truth", "ğŸ”¥ Model Prediction"])

        with tab_gt:
            st.image(disp_img, use_column_width=True, caption=f"GT Count: {len(points)}")

        with tab_pred:
            # å¦‚æœæœ‰é¢„æµ‹ç»“æœï¼Œæ˜¾ç¤ºé¢„æµ‹å›¾
            if st.session_state.pred_result is not None:
                p_count, p_overlay = st.session_state.pred_result
                st.image(p_overlay, use_column_width=True, caption=f"Predicted Count: {p_count:.2f}")
            else:
                st.info("ğŸ‘ˆ è¯·åœ¨å³ä¾§ç‚¹å‡» 'è¿è¡Œæ¨¡å‹é¢„æµ‹' æŒ‰é’®æŸ¥çœ‹ç»“æœ")

with col2:
    st.write("### ğŸ® æ§åˆ¶å°")
    c1, c2 = st.columns(2)
    with c1:
        st.button("â¬…ï¸ ä¸Šä¸€å¼ ", on_click=prev_img, use_container_width=True)
    with c2:
        st.button("ä¸‹ä¸€å¼  â¡ï¸", on_click=next_img, use_container_width=True)

    st.divider()
    st.write("### ğŸ” å¯¼èˆª")
    st.text_input("æ–‡ä»¶åæœç´¢", key="search_query", on_change=search_img)
    st.number_input(f"è·³è½¬ ID (1-{total_images})", 1, total_images, key="jump_val", on_change=jump_to_index)

    st.divider()
    st.write("### ğŸ¤– æ¨¡å‹é¢„æµ‹")

    # æ¨¡å‹è·¯å¾„è¾“å…¥
    model_path = st.text_input("æ¨¡å‹è·¯å¾„ (.pth)", value="./ckpts/shufflenet_model_best.pth")

    # è¿è¡ŒæŒ‰é’®
    if st.button("ğŸš€ è¿è¡Œæ¨¡å‹é¢„æµ‹", type="primary", use_container_width=True):
        if not os.path.exists(model_path):
            st.error(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        else:
            try:
                with st.spinner("æ­£åœ¨åŠ è½½æ¨¡å‹å¹¶æ¨ç†..."):
                    # åŠ è½½æ¨¡å‹
                    model = load_model_cached(model_path)
                    # è¿è¡Œæ¨ç†
                    p_count, p_overlay = run_inference(model, img_rgb)
                    # ä¿å­˜çŠ¶æ€
                    st.session_state.pred_result = (p_count, p_overlay)
                    # å¼ºåˆ¶åˆ·æ–°ç•Œé¢ä»¥æ˜¾ç¤ºç»“æœ
                    st.rerun()
            except Exception as e:
                st.error(f"æ¨ç†å‡ºé”™: {e}")

    # æ˜¾ç¤ºæ•°å€¼å¯¹æ¯”
    st.write("#### ğŸ“Š ç»Ÿè®¡")
    st.write(f"**GT æ•°é‡:** {len(points)}")
    if st.session_state.pred_result:
        pred_c = st.session_state.pred_result[0]
        diff = pred_c - len(points)
        color = "red" if abs(diff) > 5 else "green"
        st.write(f"**é¢„æµ‹æ•°é‡:** {pred_c:.2f}")
        st.markdown(f"**è¯¯å·®:** :{color}[{diff:+.2f}]")

    st.divider()
    st.caption(f"åˆ†è¾¨ç‡: {w}x{h}")
    st.caption(f"æ ‡æ³¨: {'âœ… å­˜åœ¨' if os.path.exists(anno_path) else 'âŒ ç¼ºå¤±'}")
